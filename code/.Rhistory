if(!all(is.na(constrained_pars))){
if(constraint_type == 'soft'){
# Soft constrained parameters: a parameter is fixed, others are conditional MLEs given constraint
constrained_pars = get_MLE_parameters(combined_profile,
constrained_pars, constrained_par_values)
}else{
stopifnot(constraint_type == 'hard')
constrained_pars = get_hard_constrained_pars(mle_pars, constrained_pars, constrained_par_values, model_par_names)
}
write.csv(tibble(par = model_par_names, global_mle = get_MLE_parameters(combined_profile),
constrained_values = constrained_pars),
paste0(plot_directory,'pars.csv'), row.names = F)
constrained_pars[is.na(constrained_pars)] <- 0
constrained_predictions <- model_function(parameters = constrained_pars, dem_plus_case_data, lineage_frequencies,
intensity_scores,cutoff_age, maternal_ab_duration, season_incidence_curves,
school_start_age, oldest_atk_rate_age, reporting_age_cutoff,
precomputed_history_probs = history_probs_constrained)
# Merge global MLE and constrained predictions into single predictions list object
predictions <- list(predictions[[1]], constrained_predictions)
names(predictions) <- c('MLE',constraint_label)
# Log-likelihood by observation year, country and lineage, for the global MLE and for the constrained mle
loglik_by_obs_year_global_MLE <- neg_loglik_function_by_obs_year(parameters = mle_pars, model_function, dem_plus_case_data,
lineage_frequencies, intensity_scores,cutoff_age, maternal_ab_duration,
season_incidence_curves, school_start_age, oldest_atk_rate_age, reporting_age_cutoff,
precomputed_history_probs = history_probs)
loglik_by_obs_year_constrained <- neg_loglik_function_by_obs_year(parameters = constrained_pars, model_function, dem_plus_case_data,
lineage_frequencies, intensity_scores,cutoff_age, maternal_ab_duration,
season_incidence_curves, school_start_age, oldest_atk_rate_age,
reporting_age_cutoff, precomputed_history_probs = history_probs_constrained)
# Comparison of log-likelihood between MLE and constrained parameters, year by year.
loglik_by_obs_year <- left_join(loglik_by_obs_year_global_MLE %>% rename(loglik_global_MLE = loglik),
loglik_by_obs_year_constrained %>% rename(loglik_constrained = loglik),
by = c('country', 'lineage', 'observation_year')) %>%
mutate(LRS = 2*(loglik_global_MLE - loglik_constrained)) %>%
mutate(p = 1 - pchisq(LRS, n_pars_constrained)) %>%
mutate(global_LRS_without_this_line = sum(LRS) - LRS)
write.csv(loglik_by_obs_year,
paste0(plot_directory,'loglik_by_obs_year.csv'), row.names = F)
write.csv(loglik_by_obs_year %>% mutate(loglik_dif = LRS/2) %>%
group_by(lineage) %>%
summarise(loglik_dif = sum(loglik_dif),
loglik_global_MLE = sum(loglik_global_MLE),
loglik_constrained = sum(loglik_constrained)),
paste0(plot_directory,'fit_comparison.csv'),
row.names = F)
}
# Plot with fraction of cases aggregated across obs. years and countries
# (We're no longer fitting to multiple countries simultaneously)
fraction_cases_pooled <- plot_pooling_countries(predictions, n_CI_replicates, CI_alpha,plot_predictions = T,
plot_fraction = T, demographic_normalization = F)
fraction_cases_pooled
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(limits = c(NULL, 2020))
?scale_x_continuous
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(limits = c(NA, 2020))
plot_directory
save_plot(paste0(plot_directory,'fraction_cases_pooled.pdf'),
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(limits = c(NA, 2020)),
base_height = 6, base_width =11)
paste0(plot_directory,'fraction_cases_pooled.pdf')
save_plot(paste0(plot_directory,'fraction_cases_pooled.pdf'),
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(limits = c(NA, 2020)),
base_height = 6, base_width =11)
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(breaks = c(1960,1980,2000,2020))
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(breaks = c(1960,1980,2000,2020),
limits = c(1958,2020))
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(breaks = c(1960,1980,2000,2020),
limits = c(1958,2020))
save_plot(paste0(plot_directory,'fraction_cases_pooled.pdf'),
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(breaks = c(1960,1980,2000,2020),
limits = c(1958,2020)),
base_height = 6, base_width =11)
case_data_path = '../results/processed_data/case_data_europe_gisaid.csv'
plot_directory
plot_directory <- '~/Desktop/NZ_pars_with_EU_data/'
subset_region <- 'Europe'
case_data <- as_tibble(read.csv(case_data_path))
case_data <- set_min_birth_year(case_data, start_birth_year)
demographic_data <- as_tibble(read.csv(demographic_data_path))
demographic_data <- normalize_demographic_data(demographic_data, start_birth_year)
intensity_scores <- as_tibble(read.csv(intensity_scores_path))
lineage_frequencies <- as_tibble(read.csv(lineage_frequencies_path))
season_incidence_curves <- as_tibble(read.csv(season_incidence_curves_path))
# Combine demographic and case data
dem_plus_case_data <- merge_data(demographic_data, case_data)
# If no precomputed history probabilities given, compute them
if(is.na(precomputed_history_probs_path)){
# Compute and export history probabilities under the MLE
R_V <- mle_pars[1]
R_Y <- mle_pars[2]
chi_V <- mle_pars[3]
chi_Y <- mle_pars[4]
gamma_VY <- mle_pars[5]
gamma_YV <-  mle_pars[6]
gamma_AV <- mle_pars[7]
gamma_AY <- mle_pars[8]
beta1 <- mle_pars[9]
beta2 <-  mle_pars[10]
beta3 <-  mle_pars[11]
if(model == 'main'){
reporting_factor_vic = mle_pars[model_par_names == 'reporting_factor']
reporting_factor_yam = mle_pars[model_par_names == 'reporting_factor']
}else{
stopifnot(model == 'two_rhos')
reporting_factor_vic = mle_pars[model_par_names == 'reporting_factor_vic']
reporting_factor_yam = mle_pars[model_par_names == 'reporting_factor_yam']
}
chi_VY = chi_Y * gamma_VY
chi_YV = chi_V * gamma_YV
chi_AV = chi_V * gamma_AV
chi_AY = chi_Y * gamma_AY
base_iprobs_tibble <- as_tibble(expand.grid(
observation_year = seq(min(dem_plus_case_data$observation_year), max(dem_plus_case_data$observation_year)),
cohort_value = seq(min(dem_plus_case_data$cohort_value), max(dem_plus_case_data$cohort_value)))) %>%
mutate(cohort_type = 'age', country = unique(dem_plus_case_data$country),
region = NA, lineage = NA, n_cases = NA, CLY_total_cases = NA, rel_pop_size = NA) %>%
arrange(observation_year)
history_probs <- calculate_iprobs(dem_plus_case_data = base_iprobs_tibble,
lineage_frequencies = lineage_frequencies,
intensity_scores = intensity_scores,
chi_VY = chi_VY, chi_YV = chi_YV,
chi_AV = chi_AV, chi_AY = chi_AY,
beta1 = beta1, beta2 = beta2, beta3 = beta3,
cutoff_age = cutoff_age,
maternal_ab_duration = maternal_ab_duration,
season_incidence_curves = season_incidence_curves,
school_start_age = school_start_age,
oldest_atk_rate_age = oldest_atk_rate_age,
birth_year_cutoff = birth_year_cutoff)
history_probs <- history_probs %>% select(country, observation_year, cohort_type, cohort_value,
matches('P_')) %>%
select(-rel_pop_size) %>% unique()
write.csv(history_probs, paste0(dirname(profiles_directory),'/MLE_history_probs.csv'), row.names = F)
history_probs_constrained <- NULL # Don't precompute for constrained parameters
}else{
# If fit was done with precomputed history probabilities (i.e., non-sentinel or sentinel specific fits)
history_probs <- as_tibble(read.csv(precomputed_history_probs_path, header = T))
history_probs_constrained <- history_probs
}
# Get model predictions
predictions <- model_function(parameters = mle_pars, dem_plus_case_data, lineage_frequencies, intensity_scores,
cutoff_age, maternal_ab_duration, season_incidence_curves, school_start_age,
oldest_atk_rate_age, reporting_age_cutoff, precomputed_history_probs = history_probs)
predictions = list(predictions)
# Predictions with constrained parameters, if any
if(!all(is.na(constrained_pars))){
if(constraint_type == 'soft'){
# Soft constrained parameters: a parameter is fixed, others are conditional MLEs given constraint
constrained_pars = get_MLE_parameters(combined_profile,
constrained_pars, constrained_par_values)
}else{
stopifnot(constraint_type == 'hard')
constrained_pars = get_hard_constrained_pars(mle_pars, constrained_pars, constrained_par_values, model_par_names)
}
write.csv(tibble(par = model_par_names, global_mle = get_MLE_parameters(combined_profile),
constrained_values = constrained_pars),
paste0(plot_directory,'pars.csv'), row.names = F)
constrained_pars[is.na(constrained_pars)] <- 0
constrained_predictions <- model_function(parameters = constrained_pars, dem_plus_case_data, lineage_frequencies,
intensity_scores,cutoff_age, maternal_ab_duration, season_incidence_curves,
school_start_age, oldest_atk_rate_age, reporting_age_cutoff,
precomputed_history_probs = history_probs_constrained)
# Merge global MLE and constrained predictions into single predictions list object
predictions <- list(predictions[[1]], constrained_predictions)
names(predictions) <- c('MLE',constraint_label)
# Log-likelihood by observation year, country and lineage, for the global MLE and for the constrained mle
loglik_by_obs_year_global_MLE <- neg_loglik_function_by_obs_year(parameters = mle_pars, model_function, dem_plus_case_data,
lineage_frequencies, intensity_scores,cutoff_age, maternal_ab_duration,
season_incidence_curves, school_start_age, oldest_atk_rate_age, reporting_age_cutoff,
precomputed_history_probs = history_probs)
loglik_by_obs_year_constrained <- neg_loglik_function_by_obs_year(parameters = constrained_pars, model_function, dem_plus_case_data,
lineage_frequencies, intensity_scores,cutoff_age, maternal_ab_duration,
season_incidence_curves, school_start_age, oldest_atk_rate_age,
reporting_age_cutoff, precomputed_history_probs = history_probs_constrained)
# Comparison of log-likelihood between MLE and constrained parameters, year by year.
loglik_by_obs_year <- left_join(loglik_by_obs_year_global_MLE %>% rename(loglik_global_MLE = loglik),
loglik_by_obs_year_constrained %>% rename(loglik_constrained = loglik),
by = c('country', 'lineage', 'observation_year')) %>%
mutate(LRS = 2*(loglik_global_MLE - loglik_constrained)) %>%
mutate(p = 1 - pchisq(LRS, n_pars_constrained)) %>%
mutate(global_LRS_without_this_line = sum(LRS) - LRS)
write.csv(loglik_by_obs_year,
paste0(plot_directory,'loglik_by_obs_year.csv'), row.names = F)
write.csv(loglik_by_obs_year %>% mutate(loglik_dif = LRS/2) %>%
group_by(lineage) %>%
summarise(loglik_dif = sum(loglik_dif),
loglik_global_MLE = sum(loglik_global_MLE),
loglik_constrained = sum(loglik_constrained)),
paste0(plot_directory,'fit_comparison.csv'),
row.names = F)
}
# Plot with fraction of cases aggregated across obs. years and countries
# (We're no longer fitting to multiple countries simultaneously)
fraction_cases_pooled <- plot_pooling_countries(predictions, n_CI_replicates, CI_alpha,plot_predictions = T,
plot_fraction = T, demographic_normalization = F)
fraction_cases_pooled
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(breaks = c(1960,1980,2000,2020),
limits = c(1958,2020))
plot_directory
save_plot(paste0(plot_directory,'fraction_cases_pooled.pdf'),
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(breaks = c(1960,1980,2000,2020),
limits = c(1958,2020)),
base_height = 6, base_width =11)
case_data_path = "../results/processed_data/case_data_china_gisaid.csv"
subset_region
subset_region <- 'Chine'
subset_region <- 'China'
plot_directory <- '~/Desktop/NZ_pars_with_China_data/'
case_data <- as_tibble(read.csv(case_data_path))
case_data <- set_min_birth_year(case_data, start_birth_year)
demographic_data <- as_tibble(read.csv(demographic_data_path))
demographic_data <- normalize_demographic_data(demographic_data, start_birth_year)
intensity_scores <- as_tibble(read.csv(intensity_scores_path))
lineage_frequencies <- as_tibble(read.csv(lineage_frequencies_path))
season_incidence_curves <- as_tibble(read.csv(season_incidence_curves_path))
# Combine demographic and case data
dem_plus_case_data <- merge_data(demographic_data, case_data)
# If no precomputed history probabilities given, compute them
if(is.na(precomputed_history_probs_path)){
# Compute and export history probabilities under the MLE
R_V <- mle_pars[1]
R_Y <- mle_pars[2]
chi_V <- mle_pars[3]
chi_Y <- mle_pars[4]
gamma_VY <- mle_pars[5]
gamma_YV <-  mle_pars[6]
gamma_AV <- mle_pars[7]
gamma_AY <- mle_pars[8]
beta1 <- mle_pars[9]
beta2 <-  mle_pars[10]
beta3 <-  mle_pars[11]
if(model == 'main'){
reporting_factor_vic = mle_pars[model_par_names == 'reporting_factor']
reporting_factor_yam = mle_pars[model_par_names == 'reporting_factor']
}else{
stopifnot(model == 'two_rhos')
reporting_factor_vic = mle_pars[model_par_names == 'reporting_factor_vic']
reporting_factor_yam = mle_pars[model_par_names == 'reporting_factor_yam']
}
chi_VY = chi_Y * gamma_VY
chi_YV = chi_V * gamma_YV
chi_AV = chi_V * gamma_AV
chi_AY = chi_Y * gamma_AY
base_iprobs_tibble <- as_tibble(expand.grid(
observation_year = seq(min(dem_plus_case_data$observation_year), max(dem_plus_case_data$observation_year)),
cohort_value = seq(min(dem_plus_case_data$cohort_value), max(dem_plus_case_data$cohort_value)))) %>%
mutate(cohort_type = 'age', country = unique(dem_plus_case_data$country),
region = NA, lineage = NA, n_cases = NA, CLY_total_cases = NA, rel_pop_size = NA) %>%
arrange(observation_year)
history_probs <- calculate_iprobs(dem_plus_case_data = base_iprobs_tibble,
lineage_frequencies = lineage_frequencies,
intensity_scores = intensity_scores,
chi_VY = chi_VY, chi_YV = chi_YV,
chi_AV = chi_AV, chi_AY = chi_AY,
beta1 = beta1, beta2 = beta2, beta3 = beta3,
cutoff_age = cutoff_age,
maternal_ab_duration = maternal_ab_duration,
season_incidence_curves = season_incidence_curves,
school_start_age = school_start_age,
oldest_atk_rate_age = oldest_atk_rate_age,
birth_year_cutoff = birth_year_cutoff)
history_probs <- history_probs %>% select(country, observation_year, cohort_type, cohort_value,
matches('P_')) %>%
select(-rel_pop_size) %>% unique()
write.csv(history_probs, paste0(dirname(profiles_directory),'/MLE_history_probs.csv'), row.names = F)
history_probs_constrained <- NULL # Don't precompute for constrained parameters
}else{
# If fit was done with precomputed history probabilities (i.e., non-sentinel or sentinel specific fits)
history_probs <- as_tibble(read.csv(precomputed_history_probs_path, header = T))
history_probs_constrained <- history_probs
}
# Get model predictions
predictions <- model_function(parameters = mle_pars, dem_plus_case_data, lineage_frequencies, intensity_scores,
cutoff_age, maternal_ab_duration, season_incidence_curves, school_start_age,
oldest_atk_rate_age, reporting_age_cutoff, precomputed_history_probs = history_probs)
predictions = list(predictions)
# Predictions with constrained parameters, if any
if(!all(is.na(constrained_pars))){
if(constraint_type == 'soft'){
# Soft constrained parameters: a parameter is fixed, others are conditional MLEs given constraint
constrained_pars = get_MLE_parameters(combined_profile,
constrained_pars, constrained_par_values)
}else{
stopifnot(constraint_type == 'hard')
constrained_pars = get_hard_constrained_pars(mle_pars, constrained_pars, constrained_par_values, model_par_names)
}
write.csv(tibble(par = model_par_names, global_mle = get_MLE_parameters(combined_profile),
constrained_values = constrained_pars),
paste0(plot_directory,'pars.csv'), row.names = F)
constrained_pars[is.na(constrained_pars)] <- 0
constrained_predictions <- model_function(parameters = constrained_pars, dem_plus_case_data, lineage_frequencies,
intensity_scores,cutoff_age, maternal_ab_duration, season_incidence_curves,
school_start_age, oldest_atk_rate_age, reporting_age_cutoff,
precomputed_history_probs = history_probs_constrained)
# Merge global MLE and constrained predictions into single predictions list object
predictions <- list(predictions[[1]], constrained_predictions)
names(predictions) <- c('MLE',constraint_label)
# Log-likelihood by observation year, country and lineage, for the global MLE and for the constrained mle
loglik_by_obs_year_global_MLE <- neg_loglik_function_by_obs_year(parameters = mle_pars, model_function, dem_plus_case_data,
lineage_frequencies, intensity_scores,cutoff_age, maternal_ab_duration,
season_incidence_curves, school_start_age, oldest_atk_rate_age, reporting_age_cutoff,
precomputed_history_probs = history_probs)
loglik_by_obs_year_constrained <- neg_loglik_function_by_obs_year(parameters = constrained_pars, model_function, dem_plus_case_data,
lineage_frequencies, intensity_scores,cutoff_age, maternal_ab_duration,
season_incidence_curves, school_start_age, oldest_atk_rate_age,
reporting_age_cutoff, precomputed_history_probs = history_probs_constrained)
# Comparison of log-likelihood between MLE and constrained parameters, year by year.
loglik_by_obs_year <- left_join(loglik_by_obs_year_global_MLE %>% rename(loglik_global_MLE = loglik),
loglik_by_obs_year_constrained %>% rename(loglik_constrained = loglik),
by = c('country', 'lineage', 'observation_year')) %>%
mutate(LRS = 2*(loglik_global_MLE - loglik_constrained)) %>%
mutate(p = 1 - pchisq(LRS, n_pars_constrained)) %>%
mutate(global_LRS_without_this_line = sum(LRS) - LRS)
write.csv(loglik_by_obs_year,
paste0(plot_directory,'loglik_by_obs_year.csv'), row.names = F)
write.csv(loglik_by_obs_year %>% mutate(loglik_dif = LRS/2) %>%
group_by(lineage) %>%
summarise(loglik_dif = sum(loglik_dif),
loglik_global_MLE = sum(loglik_global_MLE),
loglik_constrained = sum(loglik_constrained)),
paste0(plot_directory,'fit_comparison.csv'),
row.names = F)
}
# Plot with fraction of cases aggregated across obs. years and countries
# (We're no longer fitting to multiple countries simultaneously)
fraction_cases_pooled <- plot_pooling_countries(predictions, n_CI_replicates, CI_alpha,plot_predictions = T,
plot_fraction = T, demographic_normalization = F)
fraction_cases_pooled
plot_directory
save_plot(paste0(plot_directory,'fraction_cases_pooled.pdf'),
fraction_cases_pooled + theme(legend.position = 'None') +
scale_x_continuous(breaks = c(1960,1980,2000,2020),
limits = c(1958,2020)),
base_height = 6, base_width =11)
setwd("/Volumes/cobey/covid-civis/scripts")
# Run this script to download objects generated by our notebooks on the Civis platform following the instructions below:
# 1. First, you must run the notebook in the platform
#    Look for a call to the write_civis_file function associated with the name of the file you want (that call will be indicated by a Markdown cell with the file name in the notebook).
#    The number is only good for a limited time, so new numbers have to be generated when the notebook is run in the platform (e.g. to get the most recent data)
# 2. Paste the number in the download_civis command for the corresponding file below.
# If modifying this script, be careful not to export anything we shouldn't be exporting (e.g. disaggregated data)
# (Also don't export anything from Northshore, even aggregated data, except summary statistics).
library(civis)
library(rjson)
# This API key lives on Midway and is ignored by the Git repo. It expires after 30 days
# So you have to run the script from Midway.
api_key <- fromJSON(paste(readLines('../api_key.json'), collapse=""))$key
Sys.setenv(CIVIS_API_KEY = api_key)
# ================================== NOTEBOOK: "Testing data exploration" ===============================
download_civis(142742477, '../raw_data/testing_data/aggregated_testing_data.csv', overwrite = T)
# This API key lives on Midway and is ignored by the Git repo. It expires after 30 days
# So you have to run the script from Midway.
api_key <- fromJSON(paste(readLines('../api_key.json'), collapse=""))$key
Sys.setenv(CIVIS_API_KEY = api_key)
# ================================== NOTEBOOK: "Testing data exploration" ===============================
download_civis(142742477, '../raw_data/testing_data/aggregated_testing_data.csv', overwrite = T)
setwd("/Volumes/cobey/mvieira/natural_antibodies/code")
library(dplyr)
library(tidyr)
library(yaml)
library(readr)
library(stringr)
library(ggplot2)
library(cowplot)
theme_set(theme_cowplot())
args = commandArgs(trailingOnly = T)
yaml_file_paths <- list.files('../results/', pattern = 'yaml', full.names = T)
# Processes sequences ('events' in the yaml_object)
process_sequence <- function(sequence){
row <- tibble(
sequence_id = sequence$unique_ids,
length_vd_insertion = nchar(sequence$vd_insertion),
length_dj_insertion = nchar(sequence$dj_insertion),
total_insertions = nchar(sequence$vd_insertion) + nchar(sequence$dj_insertion),
v_gene = sequence$v_gene,
d_gene = sequence$d_gene,
j_gene = sequence$j_gene,
vd_insertion = sequence$vd_insertion,
dj_insertion = sequence$dj_insertion,
input_seq = sequence$input_seq
)
# If sequence has duplicates, add duplicate rows
n_duplicates <- length(sequence$duplicates[[1]])
if(n_duplicates > 0){
duplicate_names <- sequence$duplicates[[1]]
for(i in 1:n_duplicates){
row <- bind_rows(row,
row %>% mutate(sequence_id = duplicate_names[i]))
}
}
}
process_partis_output <- function(yaml_file_path){
yaml_object <- read_yaml(yaml_file_path)
return(bind_rows(lapply(yaml_object$events, FUN = process_sequence)) %>%
mutate(dataset = str_split(basename(yaml_file_path),'\\.')[[1]][1]))
}
processed_files <- lapply(as.list(yaml_file_paths), FUN = process_partis_output)
for(i in 1:length(processed_files)){
file_id = str_split(basename(yaml_file_paths[i]),'\\.')[[1]][1]
write_csv(processed_files[[i]],
paste0('../results/N-insertions_', file_id,'.csv'))
}
# Quick plot
quick_plot <- bind_rows(processed_files) %>%
select(dataset, sequence_id, length_vd_insertion, length_dj_insertion, total_insertions) %>%
pivot_longer(cols = c('length_vd_insertion', 'length_dj_insertion', 'total_insertions')) %>%
mutate(
insertion_type = case_when(
name == 'length_vd_insertion' ~ 'VD insertions',
name == 'length_dj_insertion' ~ 'DJ insertions',
name == 'total_insertions' ~ 'Total insertions'
)
) %>%
mutate(insertion_type = factor(insertion_type, levels = c('Total insertions',
'VD insertions',
'DJ insertions'))) %>%
rename(n_insertions = value) %>%
select(-name) %>%
ggplot(aes(x = dataset, y = n_insertions)) +
geom_boxplot(outlier.alpha = 0) +
geom_point(size = 2, alpha = 0.5, position = position_jitter(width = 0.1)) +
facet_grid(.~insertion_type) +
xlab('Data set') +
ylab('Number of insertions')
save_plot('../results/quick_plot.pdf', quick_plot,
base_height = 5, base_width = 12)
for(ev in yaml_object$events){
print(length(ev$duplicates[[1]]))
}
setwd("~/")
setwd("/Volumes/cobey/mvieira/natural_antibodies/code")
library(dplyr)
library(tidyr)
library(yaml)
library(readr)
library(stringr)
library(ggplot2)
library(cowplot)
theme_set(theme_cowplot())
args = commandArgs(trailingOnly = T)
yaml_file_paths <- list.files('../results/', pattern = 'yaml', full.names = T)
# Processes sequences ('events' in the yaml_object)
process_sequence <- function(sequence){
row <- tibble(
sequence_id = sequence$unique_ids,
length_vd_insertion = nchar(sequence$vd_insertion),
length_dj_insertion = nchar(sequence$dj_insertion),
total_insertions = nchar(sequence$vd_insertion) + nchar(sequence$dj_insertion),
v_gene = sequence$v_gene,
d_gene = sequence$d_gene,
j_gene = sequence$j_gene,
vd_insertion = sequence$vd_insertion,
dj_insertion = sequence$dj_insertion,
input_seq = sequence$input_seq
)
# If sequence has duplicates, add duplicate rows
n_duplicates <- length(sequence$duplicates[[1]])
if(n_duplicates > 0){
duplicate_names <- sequence$duplicates[[1]]
for(i in 1:n_duplicates){
row <- bind_rows(row,
row %>% mutate(sequence_id = duplicate_names[i]))
}
}
}
process_partis_output <- function(yaml_file_path){
yaml_object <- read_yaml(yaml_file_path)
return(bind_rows(lapply(yaml_object$events, FUN = process_sequence)) %>%
mutate(dataset = str_split(basename(yaml_file_path),'\\.')[[1]][1]))
}
processed_files <- lapply(as.list(yaml_file_paths), FUN = process_partis_output)
for(i in 1:length(processed_files)){
file_id = str_split(basename(yaml_file_paths[i]),'\\.')[[1]][1]
write_csv(processed_files[[i]],
paste0('../results/N-insertions_', file_id,'.csv'))
}
# Quick plot
quick_plot <- bind_rows(processed_files) %>%
select(dataset, sequence_id, length_vd_insertion, length_dj_insertion, total_insertions) %>%
pivot_longer(cols = c('length_vd_insertion', 'length_dj_insertion', 'total_insertions')) %>%
mutate(
insertion_type = case_when(
name == 'length_vd_insertion' ~ 'VD insertions',
name == 'length_dj_insertion' ~ 'DJ insertions',
name == 'total_insertions' ~ 'Total insertions'
)
) %>%
mutate(insertion_type = factor(insertion_type, levels = c('Total insertions',
'VD insertions',
'DJ insertions'))) %>%
rename(n_insertions = value) %>%
select(-name) %>%
ggplot(aes(x = dataset, y = n_insertions)) +
geom_boxplot(outlier.alpha = 0) +
geom_point(size = 2, alpha = 0.5, position = position_jitter(width = 0.1)) +
facet_grid(.~insertion_type) +
xlab('Data set') +
ylab('Number of insertions')
save_plot('../results/quick_plot.pdf', quick_plot,
base_height = 5, base_width = 12)
for(ev in yaml_object$events){
print(length(ev$duplicates[[1]]))
}
